{"edges": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 3], [0, 4], [0, 3], [0, 6], [1, 0], [1, 2], [1, 3], [1, 7], [1, 3], [1, 8], [2, 0], [2, 1], [2, 3], [3, 0], [3, 1], [3, 2], [3, 9], [3, 10], [3, 11], [3, 12], [3, 13], [3, 14], [3, 0], [3, 4], [3, 5], [3, 6], [3, 15], [3, 16], [3, 17], [3, 18], [3, 0], [3, 4], [3, 6], [3, 1], [3, 7], [3, 8], [4, 0], [4, 5], [4, 6], [4, 3], [4, 0], [4, 3], [4, 6], [5, 0], [5, 4], [5, 6], [5, 3], [6, 0], [6, 4], [6, 5], [6, 3], [6, 0], [6, 4], [6, 3], [3, 0], [3, 1], [3, 2], [3, 9], [3, 10], [3, 11], [3, 12], [3, 13], [3, 14], [3, 0], [3, 4], [3, 5], [3, 6], [3, 15], [3, 16], [3, 17], [3, 18], [3, 0], [3, 4], [3, 6], [3, 1], [3, 7], [3, 8], [4, 0], [4, 5], [4, 6], [4, 3], [4, 0], [4, 3], [4, 6], [3, 0], [3, 1], [3, 2], [3, 9], [3, 10], [3, 11], [3, 12], [3, 13], [3, 14], [3, 0], [3, 4], [3, 5], [3, 6], [3, 15], [3, 16], [3, 17], [3, 18], [3, 0], [3, 4], [3, 6], [3, 1], [3, 7], [3, 8], [6, 0], [6, 4], [6, 5], [6, 3], [6, 0], [6, 4], [6, 3]], "features": {"0": "0", "1": "1", "2": "2", "3": "3", "4": "4", "5": "5", "6": "6", "7": "7", "8": "8", "9": "9", "10": "10", "11": "11", "12": "12", "13": "13", "14": "14", "15": "15", "16": "16", "17": "17", "18": "18"}, "count": 3, "cite": 30, "position": 1.0, "connect": 1.6666666666666667, "totalConnect": 4.137931034482759, "totalCount": 0.42105263157894735, "totalCite": 5.5, "totalPosition": 5.125, "paper": [{"title": "Visualization and Visual Analysis of Ensemble Data: A Survey", "author": ["Wang,Junpeng", "Hazarika,Subhashis", "Li,Cheng", "Shen,Han-Wei"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2853721", "keywords": "", "abstract": "Over the last decade, ensemble visualization has witnessed a significant development due to the wide availability of ensemble data, and the increasing visualization needs from a variety of disciplines. From the data analysis point of view, it can be observed that many ensemble visualization works focus on the same facet of ensemble data, use similar data aggregation or uncertainty modeling methods. However, the lack of reflections on those essential commonalities and a systematic overview of those works prevents visualization researchers from effectively identifying new or unsolved problems and planning for further developments. In this paper, we take a holistic perspective and provide a survey of ensemble visualization. Specifically, we study ensemble visualization works in the recent decade, and categorize them from two perspectives: (1) their proposed visualization techniques; and (2) their involved analytic tasks. For the first perspective, we focus on elaborating how conventional visualization techniques (e.g., surface, volume visualization techniques) have been adapted to ensemble data; for the second perspective, we emphasize how analytic tasks (e.g., comparison, clustering) have been performed differently for ensemble data. From the study of ensemble visualization literature, we have also identified several research trends, as well as some future research opportunities.", "cite": 8, "year": "2019"}, {"title": "DeepVID: Deep Visual Interpretation and Diagnosis for Image Classifiers via Knowledge Distillation", "author": ["Wang,Junpeng", "Gou,Liang", "Zhang,Wei", "Yang,Hao", "Shen,Han-Wei"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2019.2903943", "keywords": "", "abstract": "Deep Neural Networks (DNNs) have been extensively used in multiple disciplines due to their superior performance. However, in most cases, DNNs are considered as black-boxes and the interpretation of their internal working mechanism is usually challenging. Given that model trust is often built on the understanding of how a model works, the interpretation of DNNs becomes more important, especially in safety-critical applications (e.g., medical diagnosis, autonomous driving). In this paper, we propose DeepVID, a Deep learning approach to Visually Interpret and Diagnose DNN models, especially image classifiers. In detail, we train a small locally-faithful model to mimic the behavior of an original cumbersome DNN around a particular data instance of interest, and the local model is sufficiently simple such that it can be visually interpreted (e.g., a linear model). Knowledge distillation is used to transfer the knowledge from the cumbersome DNN to the small model, and a deep generative model (i.e., variational auto-encoder) is used to generate neighbors around the instance of interest. Those neighbors, which come with small feature variances and semantic meanings, can effectively probe the DNN's behaviors around the interested instance and help the small model to learn those behaviors. Through comprehensive evaluations, as well as case studies conducted together with deep learning experts, we validate the effectiveness of DeepVID.", "cite": 10, "year": "2019"}, {"title": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks", "author": ["Wang,Junpeng", "Gou,Liang", "Shen,Han-Wei", "Yang,Hao"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2864504", "keywords": "", "abstract": "Deep Q-Network (DON). as one type of deep reinforcement learning model. targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand. diagnose, and potentially improve DQN models.", "cite": 12, "year": "2019"}], "name": "Wang,Junpeng", "nodes": [{"id": "0"}, {"id": "1"}, {"id": "2"}, {"id": "3"}, {"id": "4"}, {"id": "5"}, {"id": "6"}, {"id": "7"}, {"id": "8"}, {"id": "9"}, {"id": "10"}, {"id": "11"}, {"id": "12"}, {"id": "13"}, {"id": "14"}, {"id": "15"}, {"id": "16"}, {"id": "17"}, {"id": "18"}], "edgess": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "0", "target": "5"}, {"source": "0", "target": "6"}, {"source": "1", "target": "2"}, {"source": "1", "target": "3"}, {"source": "1", "target": "7"}, {"source": "1", "target": "8"}, {"source": "2", "target": "3"}, {"source": "3", "target": "9"}, {"source": "3", "target": "10"}, {"source": "3", "target": "11"}, {"source": "3", "target": "12"}, {"source": "3", "target": "13"}, {"source": "3", "target": "14"}, {"source": "3", "target": "4"}, {"source": "3", "target": "5"}, {"source": "3", "target": "6"}, {"source": "3", "target": "15"}, {"source": "3", "target": "16"}, {"source": "3", "target": "17"}, {"source": "3", "target": "18"}, {"source": "3", "target": "7"}, {"source": "3", "target": "8"}, {"source": "4", "target": "5"}, {"source": "4", "target": "6"}, {"source": "5", "target": "6"}], "year": 2019, "authorId": {"0": "Wang,Junpeng", "1": "Hazarika,Subhashis", "2": "Li,Cheng", "3": "Shen,Han-Wei", "4": "Gou,Liang", "5": "Zhang,Wei", "6": "Yang,Hao", "7": "Dutta,Soumya", "8": "Chen,Jen-Ping", "9": "Guo,Hanqi", "10": "He,Wenbin", "11": "Seo,Sangmin", "12": "Constantinescu,EmilMihai", "13": "Liu,Chunhui", "14": "Peterka,Tom", "15": "Ji,Xiaonan", "16": "Ritter,Alan", "17": "Machiraju,Raghu", "18": "Yen,Po-Yin"}}