{"edges": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 2], [1, 0], [1, 3], [1, 4], [2, 1], [2, 0], [2, 3], [2, 4], [3, 1], [3, 2], [3, 0], [3, 4], [4, 5], [4, 6], [4, 7], [4, 8], [4, 1], [4, 2], [4, 0], [4, 3]], "features": {"0": "0", "1": "1", "2": "2", "3": "3", "4": "4", "5": "5", "6": "6", "7": "7", "8": "8"}, "count": 1, "cite": 1, "position": 3.0, "connect": 1.0, "totalConnect": 1.7142857142857142, "totalCount": 0.2222222222222222, "totalCite": 1.5, "totalPosition": 5.0, "paper": [{"title": "Learning Discriminative 3D Shape Representations by View Discerning Networks", "author": ["Leng,Biao", "Zhang,Cheng", "Zhou,Xiaocheng", "Xu,Cheng", "Xu,Kai"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2865317", "keywords": "", "abstract": "In view-based 3D shape recognition, extracting discriminative visual representation of 3D shapes from projected images is considered the core problem. Projections with low discriminative ability can adversely influence the final 3D shape representation. Especially under the real situations with background clutter and object occlusion, the adverse effect is even more severe. To resolve this problem, we propose a novel deep neural network, View Discerning Network, which learns to judge the quality of views and adjust their contributions to the representation of shapes. In this network, a Score Generation Unit is devised to evaluate the quality of each projected image with score vectors. These score vectors are used to weight the image features and the weighted features perform much better than original features in 3D shape recognition task. In particular, we introduce two structures of Score Generation Unit, Channel-wise Score Unit and Part-wise Score Unit, to assess the quality of feature maps from different perspectives. Our network aggregates features and scores in an end-to-end framework, so that final shape descriptors are directly obtained from its output. Our experiments on ModelNet and ShapeNet Core55 show that View Discerning Network outperforms the state-of-the-arts in terms of the retrieval task, with excellent robustness against background clutter and object occlusion.", "cite": 1, "year": "2019"}], "name": "Zhou,Xiaocheng", "nodes": [{"id": "0"}, {"id": "1"}, {"id": "2"}, {"id": "3"}, {"id": "4"}, {"id": "5"}, {"id": "6"}, {"id": "7"}, {"id": "8"}], "edgess": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "1", "target": "2"}, {"source": "1", "target": "3"}, {"source": "1", "target": "4"}, {"source": "2", "target": "3"}, {"source": "2", "target": "4"}, {"source": "3", "target": "4"}, {"source": "4", "target": "5"}, {"source": "4", "target": "6"}, {"source": "4", "target": "7"}, {"source": "4", "target": "8"}], "year": 2019, "authorId": {"0": "Zhou,Xiaocheng", "1": "Leng,Biao", "2": "Zhang,Cheng", "3": "Xu,Cheng", "4": "Xu,Kai", "5": "Chen,Songle", "6": "Zheng,Lintao", "7": "Zhang,Yan", "8": "Sun,Zhixin"}}