{"edges": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [0, 10], [0, 1], [0, 11], [0, 12], [0, 13], [0, 14], [0, 15], [0, 16], [0, 17], [0, 18], [0, 19], [0, 20], [0, 21], [0, 22], [0, 23], [0, 24], [0, 25], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 0], [1, 8], [1, 9], [1, 10], [1, 11], [1, 0], [1, 12], [2, 1], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 0], [2, 8], [2, 26], [2, 8], [2, 27], [3, 1], [3, 2], [3, 4], [3, 5], [3, 6], [3, 7], [3, 0], [3, 8], [4, 1], [4, 2], [4, 3], [4, 5], [4, 6], [4, 7], [4, 0], [4, 8], [5, 1], [5, 2], [5, 3], [5, 4], [5, 6], [5, 7], [5, 0], [5, 8], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 7], [6, 0], [6, 8], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6], [7, 0], [7, 8], [8, 1], [8, 2], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7], [8, 0], [8, 2], [8, 26], [8, 27], [8, 26], [8, 28], [8, 29], [8, 30], [9, 10], [9, 1], [9, 11], [9, 0], [9, 12], [10, 9], [10, 1], [10, 11], [10, 0], [10, 12], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 0], [1, 8], [1, 9], [1, 10], [1, 11], [1, 0], [1, 12], [11, 9], [11, 10], [11, 1], [11, 0], [11, 12], [12, 9], [12, 10], [12, 1], [12, 11], [12, 0], [13, 14], [13, 0], [14, 13], [14, 0], [15, 16], [15, 17], [15, 18], [15, 19], [15, 20], [15, 0], [16, 15], [16, 17], [16, 18], [16, 19], [16, 20], [16, 0], [17, 15], [17, 16], [17, 18], [17, 19], [17, 20], [17, 0], [18, 31], [18, 32], [18, 33], [18, 34], [18, 35], [18, 36], [18, 37], [18, 38], [18, 39], [18, 40], [18, 15], [18, 16], [18, 17], [18, 19], [18, 20], [18, 0], [19, 15], [19, 16], [19, 17], [19, 18], [19, 20], [19, 0], [20, 15], [20, 16], [20, 17], [20, 18], [20, 19], [20, 0], [21, 22], [21, 23], [21, 24], [21, 0], [21, 25], [22, 21], [22, 23], [22, 24], [22, 0], [22, 25], [23, 21], [23, 22], [23, 24], [23, 0], [23, 25], [24, 21], [24, 22], [24, 23], [24, 0], [24, 25], [25, 21], [25, 22], [25, 23], [25, 24], [25, 0], [25, 41], [25, 42], [25, 43]], "features": {"0": "0", "1": "1", "2": "2", "3": "3", "4": "4", "5": "5", "6": "6", "7": "7", "8": "8", "9": "9", "10": "10", "11": "11", "12": "12", "13": "13", "14": "14", "15": "15", "16": "16", "17": "17", "18": "18", "19": "19", "20": "20", "21": "21", "22": "22", "23": "23", "24": "24", "25": "25", "26": "26", "27": "27", "28": "28", "29": "29", "30": "30", "31": "31", "32": "32", "33": "33", "34": "34", "35": "35", "36": "36", "37": "37", "38": "38", "39": "39", "40": "40", "41": "41", "42": "42", "43": "43"}, "count": 5, "cite": 44, "position": 5.6, "connect": 1.04, "totalConnect": 1.981651376146789, "totalCount": 0.45454545454545453, "totalCite": 7.2, "totalPosition": 5.9, "paper": [{"title": "Commercial Visual Analytics Systems-Advances in the Big Data Analytics Field", "author": ["Behrisch,Michael", "Streeb,Dirk", "Stoffel,Florian", "Seebacher,Daniel", "Matejek,Brian", "Weber,StefanHagen", "Mittelstaedt,Sebastian", "Pfister,Hanspeter", "Keim,Daniel"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2859973", "keywords": "", "abstract": "Five years after the first state-of-the-art report on Commercial Visual Analytics Systems we present a reevaluation of the Big Data Analytics field. We build on the success of the 2012 survey, which was influential even beyond the boundaries of the InfoVis and Visual Analytics (VA) community. While the field has matured significantly since the original survey, we find that innovation and research-driven development are increasingly sacrificed to satisfy a wide range of user groups. We evaluate new product versions on established evaluation criteria, such as available features, performance, and usability, to extend on and assure comparability with the previous survey. We also investigate previously unavailable products to paint a more complete picture of the commercial VA landscape. Furthermore, we introduce novel measures, like suitability for specific user groups and the ability to handle complex data types, and undertake a new case study to highlight innovative features. We explore the achievements in the commercial sector in addressing VA challenges and propose novel developments that should be on systems' roadmaps in the coming years.", "cite": 9, "year": "2019"}, {"title": "SEQ2SEQ-VIS : A Visual Debugging Tool for Sequence-to-Sequence Models", "author": ["Strobelt,Hendrik", "Gehrmann,Sebastian", "Behrisch,Michael", "Perer,Adam", "Pfister,Hanspeter", "Rush,AlexanderM."], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2865044", "keywords": "", "abstract": "Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and \"what if\"-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned. to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.", "cite": 15, "year": "2019"}, {"title": "Evaluating 'Graphical Perception' with CNNs", "author": ["Haehn,Daniel", "Tompkin,James", "Pfister,Hanspeter"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2865138", "keywords": "", "abstract": "Convolutional neural networks can successfully perform many computer vision tasks on images. For visualization, how do CNNs perform when applied to graphical perception tasks? We investigate this question by reproducing Cleveland and McGill's seminal 1984 experiments, which measured human perception efficiency of different visual encodings and defined elementary perceptual tasks for visualization. We measure the graphical perceptual capabilities of four network architectures on five different visualization tasks and compare to existing and new human performance baselines. While under limited circumstances CNNs are able to meet or outperform human task performance we find that CNNs are not currently a good model for human graphical perception. We present the results of these experiments to foster the understanding of how CNNs succeed and fail when applied to data visualizations.", "cite": 3, "year": "2019"}, {"title": "DXR: A Toolkit for Building Immersive Data Visualizations", "author": ["Sicat,Ronell", "Li,Jiabao", "Choi,JunYoung", "Cordeil,Maxime", "Jeong,Won-Ki", "Bach,Benjamin", "Pfister,Hanspeter"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2865152", "keywords": "", "abstract": "This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR. developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e.. while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate the flexibility of DXR through several examples spanning a wide range of applications.", "cite": 15, "year": "2019"}, {"title": "Culling for Extreme-Scale Segmentation Volumes: A Hybrid Deterministic and Probabilistic Approach", "author": ["Beyer,Johanna", "Mohammed,Haneen", "Agus,Marco", "Al-Awami,AliK.", "Pfister,Hanspeter", "Hadwiger,Markus"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2864847", "keywords": "", "abstract": "With the rapid increase in raw volume data sizes, such as terabyte-sized microscopy volumes, the corresponding segmentation label volumes have become extremely large as well. We focus on integer label data, whose efficient representation in memory, as well as fast random data access, pose an even greater challenge than the raw image data. Often, it is crucial to be able to rapidly identify which segments are located where, whether for empty space skipping for fast rendering, or for spatial proximity queries. We refer to this process as culling. In order to enable efficient culling of millions of labeled segments, we present a novel hybrid approach that combines deterministic and probabilistic representations of label data in a data-adaptive hierarchical data structure that we call the label list tree. In each node, we adaptively encode label data using either a probabilistic constant-time access representation for fast conservative culling, or a deterministic logarithmic-time access representation for exact queries. We choose the best data structures for representing the labels of each spatial region while building the label list tree. At run time, we further employ a novel query-adaptive culling strategy. While filtering a query down the tree, we prune it successively, and in each node adaptively select the representation that is best suited for evaluating the pruned query, depending on its size. We show an analysis of the efficiency of our approach with several large data sets from connectomics, including a brain scan with more than 13 million labeled segments, and compare our method to conventional culling approaches. Our approach achieves significant reductions in storage size as well as faster query times.", "cite": 2, "year": "2019"}], "name": "Pfister,Hanspeter", "nodes": [{"id": "0"}, {"id": "1"}, {"id": "2"}, {"id": "3"}, {"id": "4"}, {"id": "5"}, {"id": "6"}, {"id": "7"}, {"id": "8"}, {"id": "9"}, {"id": "10"}, {"id": "11"}, {"id": "12"}, {"id": "13"}, {"id": "14"}, {"id": "15"}, {"id": "16"}, {"id": "17"}, {"id": "18"}, {"id": "19"}, {"id": "20"}, {"id": "21"}, {"id": "22"}, {"id": "23"}, {"id": "24"}, {"id": "25"}, {"id": "26"}, {"id": "27"}, {"id": "28"}, {"id": "29"}, {"id": "30"}, {"id": "31"}, {"id": "32"}, {"id": "33"}, {"id": "34"}, {"id": "35"}, {"id": "36"}, {"id": "37"}, {"id": "38"}, {"id": "39"}, {"id": "40"}, {"id": "41"}, {"id": "42"}, {"id": "43"}], "edgess": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "0", "target": "5"}, {"source": "0", "target": "6"}, {"source": "0", "target": "7"}, {"source": "0", "target": "8"}, {"source": "0", "target": "9"}, {"source": "0", "target": "10"}, {"source": "0", "target": "11"}, {"source": "0", "target": "12"}, {"source": "0", "target": "13"}, {"source": "0", "target": "14"}, {"source": "0", "target": "15"}, {"source": "0", "target": "16"}, {"source": "0", "target": "17"}, {"source": "0", "target": "18"}, {"source": "0", "target": "19"}, {"source": "0", "target": "20"}, {"source": "0", "target": "21"}, {"source": "0", "target": "22"}, {"source": "0", "target": "23"}, {"source": "0", "target": "24"}, {"source": "0", "target": "25"}, {"source": "1", "target": "2"}, {"source": "1", "target": "3"}, {"source": "1", "target": "4"}, {"source": "1", "target": "5"}, {"source": "1", "target": "6"}, {"source": "1", "target": "7"}, {"source": "1", "target": "8"}, {"source": "1", "target": "9"}, {"source": "1", "target": "10"}, {"source": "1", "target": "11"}, {"source": "1", "target": "12"}, {"source": "2", "target": "3"}, {"source": "2", "target": "4"}, {"source": "2", "target": "5"}, {"source": "2", "target": "6"}, {"source": "2", "target": "7"}, {"source": "2", "target": "8"}, {"source": "2", "target": "26"}, {"source": "2", "target": "27"}, {"source": "3", "target": "4"}, {"source": "3", "target": "5"}, {"source": "3", "target": "6"}, {"source": "3", "target": "7"}, {"source": "3", "target": "8"}, {"source": "4", "target": "5"}, {"source": "4", "target": "6"}, {"source": "4", "target": "7"}, {"source": "4", "target": "8"}, {"source": "5", "target": "6"}, {"source": "5", "target": "7"}, {"source": "5", "target": "8"}, {"source": "6", "target": "7"}, {"source": "6", "target": "8"}, {"source": "7", "target": "8"}, {"source": "8", "target": "26"}, {"source": "8", "target": "27"}, {"source": "8", "target": "28"}, {"source": "8", "target": "29"}, {"source": "8", "target": "30"}, {"source": "9", "target": "10"}, {"source": "9", "target": "11"}, {"source": "9", "target": "12"}, {"source": "10", "target": "11"}, {"source": "10", "target": "12"}, {"source": "11", "target": "12"}, {"source": "13", "target": "14"}, {"source": "15", "target": "16"}, {"source": "15", "target": "17"}, {"source": "15", "target": "18"}, {"source": "15", "target": "19"}, {"source": "15", "target": "20"}, {"source": "16", "target": "17"}, {"source": "16", "target": "18"}, {"source": "16", "target": "19"}, {"source": "16", "target": "20"}, {"source": "17", "target": "18"}, {"source": "17", "target": "19"}, {"source": "17", "target": "20"}, {"source": "18", "target": "31"}, {"source": "18", "target": "32"}, {"source": "18", "target": "33"}, {"source": "18", "target": "34"}, {"source": "18", "target": "35"}, {"source": "18", "target": "36"}, {"source": "18", "target": "37"}, {"source": "18", "target": "38"}, {"source": "18", "target": "39"}, {"source": "18", "target": "40"}, {"source": "18", "target": "19"}, {"source": "18", "target": "20"}, {"source": "19", "target": "20"}, {"source": "21", "target": "22"}, {"source": "21", "target": "23"}, {"source": "21", "target": "24"}, {"source": "21", "target": "25"}, {"source": "22", "target": "23"}, {"source": "22", "target": "24"}, {"source": "22", "target": "25"}, {"source": "23", "target": "24"}, {"source": "23", "target": "25"}, {"source": "24", "target": "25"}, {"source": "25", "target": "41"}, {"source": "25", "target": "42"}, {"source": "25", "target": "43"}], "year": 2019, "authorId": {"0": "Pfister,Hanspeter", "1": "Behrisch,Michael", "2": "Streeb,Dirk", "3": "Stoffel,Florian", "4": "Seebacher,Daniel", "5": "Matejek,Brian", "6": "Weber,StefanHagen", "7": "Mittelstaedt,Sebastian", "8": "Keim,Daniel", "9": "Strobelt,Hendrik", "10": "Gehrmann,Sebastian", "11": "Perer,Adam", "12": "Rush,AlexanderM.", "13": "Haehn,Daniel", "14": "Tompkin,James", "15": "Sicat,Ronell", "16": "Li,Jiabao", "17": "Choi,JunYoung", "18": "Cordeil,Maxime", "19": "Jeong,Won-Ki", "20": "Bach,Benjamin", "21": "Beyer,Johanna", "22": "Mohammed,Haneen", "23": "Agus,Marco", "24": "Al-Awami,AliK.", "25": "Hadwiger,Markus", "26": "El-Assady,Mennatallah", "27": "Chen,Min", "28": "Sperrle,Fabian", "29": "Deussen,Oliver", "30": "Collins,Christopher", "31": "Yang,Yalong", "32": "Dwyer,Tim", "33": "Jenny,Bernhard", "34": "Marriott,Kim", "35": "Chen,Haohui", "36": "Hurter,Christophell", "37": "Riche,NathalieHenry", "38": "Drucker,StevenM.", "39": "Alligier,Richard", "40": "Vuillemot,Romain", "41": "Mlejnek,Matej", "42": "Theussl,Thomas", "43": "Rautek,Peter"}}