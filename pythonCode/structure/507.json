{"edges": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [1, 2], [1, 3], [1, 0], [2, 1], [2, 3], [2, 0], [3, 1], [3, 2], [3, 0], [4, 5], [4, 6], [4, 7], [4, 8], [4, 0], [4, 9], [5, 4], [5, 6], [5, 7], [5, 8], [5, 0], [5, 9], [6, 4], [6, 5], [6, 7], [6, 8], [6, 0], [6, 9], [7, 10], [7, 11], [7, 12], [7, 4], [7, 5], [7, 6], [7, 8], [7, 0], [7, 9], [8, 4], [8, 5], [8, 6], [8, 7], [8, 0], [8, 9], [9, 4], [9, 5], [9, 6], [9, 7], [9, 8], [9, 0]], "features": {"0": "0", "1": "1", "2": "2", "3": "3", "4": "4", "5": "5", "6": "6", "7": "7", "8": "8", "9": "9", "10": "10", "11": "11", "12": "12"}, "count": 2, "cite": 6, "position": 5.0, "connect": 1.0, "totalConnect": 1.9, "totalCount": 0.23076923076923078, "totalCite": 2.6666666666666665, "totalPosition": 5.0, "paper": [{"title": "Context-Aware Asset Search for Graphic Design", "author": ["Kovacs,Balazs", "O'Donovan,Peter", "Bala,Kavita", "Hertzmann,Aaron"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2842734", "keywords": "", "abstract": "Graphic design tools provide powerful controls for expert-level design creation, but the options can often be overwhelming for novices. This paper proposes Context-Aware Asset Search tools that take the current state of the user's design into account, thereby providing search and selections that are compatible with the current design and better fit the user's needs. In particular, we focus on image search and color selection, two tasks that are central to design. We learn a model for compatibility of images and colors within a design, using crowdsourced data. We then use the learned model to rank image search results or color suggestions during design. We found counterintuitive behavior using conventional training with pairwise comparisons for image search, where models with and without compatibility performed similarly. We describe a data collection procedure that alleviates this problem. We show that our method outperforms baseline approaches in quantitative evaluation, and we also evaluate a prototype interactive design tool.", "cite": 0, "year": "2019"}, {"title": "Motion parallax for 360 degrees RGBD video", "author": ["Serrano,Ana", "Kim,Incheol", "Chen,Zhili", "DiVerdi,Stephen", "Gutierrez,Diego", "Hertzmann,Aaron", "Masia,Belen"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2019.2898757", "keywords": "", "abstract": "We present a method for adding parallax and real-time playback of 360 degrees videos in Virtual Reality headsets. In current video players, the playback does not respond to translational head movement, which reduces the feeling of immersion, and causes motion sickness for some viewers. Given a 360 degrees video and its corresponding depth (provided by current stereo 360 degrees stitching algorithms), a naive image-based rendering approach would use the depth to generate a 3D mesh around the viewer, then translate it appropriately as the viewer moves their head. However, this approach breaks at depth discontinuities, showing visible distortions, whereas cutting the mesh at such discontinuities leads to ragged silhouettes and holes at disocclusions. We address these issues by improving the given initial depth map to yield cleaner, more natural silhouettes. We rely on a three-layer scene representation, made up of a foreground layer and two static background layers, to handle disocclusions by propagating information from multiple frames for the first background layer, and then inpainting for the second one. Our system works with input from many of today's most popular 360 degrees stereo capture devices (e.g., Yi Halo or GoPro Odyssey), and works well even if the original video does not provide depth information. Our user studies confirm that our method provides a more compelling viewing experience than without parallax, increasing immersion while reducing discomfort and nausea.", "cite": 6, "year": "2019"}], "name": "Hertzmann,Aaron", "nodes": [{"id": "0"}, {"id": "1"}, {"id": "2"}, {"id": "3"}, {"id": "4"}, {"id": "5"}, {"id": "6"}, {"id": "7"}, {"id": "8"}, {"id": "9"}, {"id": "10"}, {"id": "11"}, {"id": "12"}], "edgess": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "0", "target": "5"}, {"source": "0", "target": "6"}, {"source": "0", "target": "7"}, {"source": "0", "target": "8"}, {"source": "0", "target": "9"}, {"source": "1", "target": "2"}, {"source": "1", "target": "3"}, {"source": "2", "target": "3"}, {"source": "4", "target": "5"}, {"source": "4", "target": "6"}, {"source": "4", "target": "7"}, {"source": "4", "target": "8"}, {"source": "4", "target": "9"}, {"source": "5", "target": "6"}, {"source": "5", "target": "7"}, {"source": "5", "target": "8"}, {"source": "5", "target": "9"}, {"source": "6", "target": "7"}, {"source": "6", "target": "8"}, {"source": "6", "target": "9"}, {"source": "7", "target": "10"}, {"source": "7", "target": "11"}, {"source": "7", "target": "12"}, {"source": "7", "target": "8"}, {"source": "7", "target": "9"}, {"source": "8", "target": "9"}], "year": 2019, "authorId": {"0": "Hertzmann,Aaron", "1": "Kovacs,Balazs", "2": "O'Donovan,Peter", "3": "Bala,Kavita", "4": "Serrano,Ana", "5": "Kim,Incheol", "6": "Chen,Zhili", "7": "DiVerdi,Stephen", "8": "Gutierrez,Diego", "9": "Masia,Belen", "10": "Tan,Jianchao", "11": "Lu,Jingwan", "12": "Gingold,Yotam"}}