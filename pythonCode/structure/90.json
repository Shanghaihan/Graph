{"edges": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 2], [0, 6], [0, 4], [0, 7], [0, 8], [0, 9], [0, 10], [0, 11], [0, 12], [0, 13], [0, 14], [0, 15], [0, 16], [0, 17], [0, 18], [0, 19], [0, 20], [1, 2], [1, 3], [1, 4], [1, 0], [2, 1], [2, 3], [2, 4], [2, 0], [2, 5], [2, 6], [2, 4], [2, 0], [3, 1], [3, 2], [3, 4], [3, 0], [4, 1], [4, 2], [4, 3], [4, 0], [4, 5], [4, 2], [4, 6], [4, 0], [5, 2], [5, 6], [5, 4], [5, 0], [2, 1], [2, 3], [2, 4], [2, 0], [2, 5], [2, 6], [2, 4], [2, 0], [6, 5], [6, 2], [6, 4], [6, 0], [4, 1], [4, 2], [4, 3], [4, 0], [4, 5], [4, 2], [4, 6], [4, 0], [7, 8], [7, 9], [7, 10], [7, 0], [7, 11], [8, 7], [8, 9], [8, 10], [8, 0], [8, 11], [9, 7], [9, 8], [9, 10], [9, 0], [9, 11], [10, 7], [10, 8], [10, 9], [10, 0], [10, 11], [10, 21], [10, 22], [11, 7], [11, 8], [11, 9], [11, 10], [11, 0], [12, 13], [12, 0], [12, 14], [13, 12], [13, 0], [13, 14], [14, 12], [14, 13], [14, 0], [15, 16], [15, 17], [15, 18], [15, 0], [16, 15], [16, 17], [16, 18], [16, 0], [17, 23], [17, 24], [17, 25], [17, 18], [17, 15], [17, 16], [17, 18], [17, 0], [18, 23], [18, 24], [18, 17], [18, 25], [18, 15], [18, 16], [18, 17], [18, 0], [19, 20], [19, 0], [20, 19], [20, 0]], "features": {"0": "0", "1": "1", "2": "2", "3": "3", "4": "4", "5": "5", "6": "6", "7": "7", "8": "8", "9": "9", "10": "10", "11": "11", "12": "12", "13": "13", "14": "14", "15": "15", "16": "16", "17": "17", "18": "18", "19": "19", "20": "20", "21": "21", "22": "22", "23": "23", "24": "24", "25": "25"}, "count": 6, "cite": 11, "position": 4.333333333333333, "connect": 1.1, "totalConnect": 2.2711864406779663, "totalCount": 0.34615384615384615, "totalCite": 1.7777777777777777, "totalPosition": 4.333333333333333, "paper": [{"title": "Modeling Data-Driven Dominance Traits for Virtual Characters using Gait Analysis.", "author": ["Randhavane,TanmayV", "Bera,Aniket", "Kubin,Emily", "Gray,Kurt", "Manocha,Dinesh"], "soname": "IEEE transactions on visualization and computer graphics", "DOI": "10.1109/TVCG.2019.2953063", "keywords": "", "abstract": "We present a data-driven algorithm for generating gaits of virtual characters with varying dominance traits. Our formulation utilizes a user study to establish a data-driven dominance mapping between gaits and dominance labels. We use our dominance mapping to generate walking gaits for virtual characters that exhibit a variety of dominance traits while interacting with the user. Furthermore, we extract gait features based on known criteria in visual perception and psychology literature that can be used to identify the dominance levels of any walking gait. We validate our mapping and the perceived dominance traits by a second user study in an immersive virtual environment. Our gait dominance classification algorithm can classify the dominance traits of gaits with ~73% accuracy. We also present an application of our approach that simulates interpersonal relationships between virtual characters. To the best of our knowledge, ours is the first practical approach to classifying gait dominance and generate dominance traits in virtual characters.", "cite": 0, "year": "2019"}, {"title": "FVA: Modeling Perceived Friendliness of Virtual Agents Using Movement Characteristics", "author": ["Randhavane,Tanmay", "Bera,Aniket", "Kapsaskis,Kyra", "Gray,Kurt", "Manocha,Dinesh"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2019.2932235", "keywords": "", "abstract": "We present a new approach for improving the friendliness and warmth of a virtual agent in an AR environment by generating appropriate movement characteristics. Our algorithm is based on a novel data-driven friendliness model that is computed using a user-study and psychological characteristics. We use our model to control the movements corresponding to the gaits, gestures, and gazing of friendly virtual agents (FVAs) as they interact with the user's avatar and other agents in the environment. We have integrated FVA agents with an AR environment using with a Microsoft HoloLens. Our algorithm can generate plausible movements at interactive rates to increase the social presence. We also investigate the perception of a user in an AR setting and observe that an FVA has a statistically significant improvement in terms of the perceived friendliness and social presence of a user compared to an agent without the friendliness modeling. We observe an increment of 5.71% in the mean responses to a friendliness measure and an improvement of 4.03% in the mean responses to a social presence measure.", "cite": 1, "year": "2019"}, {"title": "Heter-Sim: Heterogeneous Multi-Agent Systems Simulation by Interactive Data-Driven Optimization.", "author": ["Ren,Jiaping", "Xiang,Wei", "Xiao,Yangxi", "Yang,Ruigang", "Manocha,Dinesh", "Jin,Xiaogang"], "soname": "IEEE transactions on visualization and computer graphics", "DOI": "10.1109/TVCG.2019.2946769", "keywords": "", "abstract": "Interactive multi-agent simulation algorithms are used to compute the trajectories and behaviors of different entities in virtual reality scenarios. However, current methods involve considerable parameter tweaking to generate plausible behaviors. We introduce a novel approach (Heter-Sim) that combines physics-based simulation methods with data-driven techniques using an optimization-based formulation. Our approach is general and can simulate heterogeneous agents corresponding to human crowds, traffic, vehicles, or combinations of different agents with varying dynamics. We estimate motion states from real-world datasets that include information about position, velocity, and control direction. Our optimization algorithm considers several constraints, including velocity continuity, collision avoidance, attraction, direction control. Other constraints are implemented by introducing a novel energy function to control the motions of heterogeneous agents. To accelerate the computations, we reduce the search space for both collision avoidance and optimal solution computation. Heter-Sim can simulate tens or hundreds of agents at interactive rates and we compare its accuracy with real-world datasets and prior algorithms. We also perform user studies that evaluate the plausible behaviors generated by our algorithm and a user study that evaluates the plausibility of our algorithm via VR.", "cite": 3, "year": "2019"}, {"title": "Realtime Hand-Object Interaction Using Learned Grasp Space for Virtual Environments", "author": ["Tian,Hao", "Wang,Changbo", "Manocha,Dinesh", "Zhang,Xinyu"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2849381", "keywords": "", "abstract": "We present a realtime virtual grasping algorithm to model interactions with virtual objects. Our approach is designed for multi-fingered hands and makes no assumptions about the motion of the user's hand or the virtual objects. Given a model of the virtual hand, we use machine learning and particle swarm optimization to automatically pre-compute stable grasp configurations for that object. The learning pre-computation step is accelerated using GPU parallelization. At runtime, we rely on the pre-computed stable grasp configurations, and dynamics/non-penetration constraints along with motion planning techniques to compute plausible looking grasps. In practice, our realtime algorithm can perform virtual grasping operations in less than 20ms for complex virtual objects, including high genus objects with holes. We have integrated our grasping algorithm with Oculus Rift HMD and Leap Motion controller and evaluated its performance for different tasks corresponding to grabbing virtual objects and placing them at arbitrary locations. Our user evaluation suggests that our virtual grasping algorithm can increase the user's realism and participation in these tasks and offers considerable benefits over prior interaction algorithms, such as pinch grasping and raycast picking.", "cite": 1, "year": "2019"}, {"title": "SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction", "author": ["Hu,Zhiming", "Zhang,Congyi", "Li,Sheng", "Wang,Guoping", "Manocha,Dinesh"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2019.2899187", "keywords": "", "abstract": "We present a novel, data-driven eye-head coordination model that can be used for realtime gaze prediction for immersive HMD-based applications without any external hardware or eye tracker. Our model (SGaze) is computed by generating a large dataset that corresponds to different users navigating in virtual worlds with different lighting conditions. We perform statistical analysis on the recorded data and observe a linear correlation between gaze positions and head rotation angular velocities. We also find that there exists a latency between eye movements and head movements. SGaze can work as a software-based realtime gaze predictor and we formulate a time related function between head movement and eye movement and use that for realtime gaze position prediction. We demonstrate the benefits of SGaze for gaze-contingent rendering and evaluate the results with a user study.", "cite": 5, "year": "2019"}, {"title": "Inferring User Intent using Bayesian Theory of Mind in Shared Avatar-Agent Virtual Environments", "author": ["Narang,Sahil", "Best,Andrew", "Manocha,Dinesh"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2019.2898800", "keywords": "", "abstract": "We present a real-time algorithm to infer the intention of a user's avatar in a virtual environment shared with multiple human-like agents. Our algorithm applies the Bayesian Theory of Mind approach to make inferences about the avatar's hidden intentions based on the observed proxemics and gaze-based cues. Our approach accounts for the potential irrationality in human behavior, as well as the dynamic nature of an individual's intentions. The inferred intent is used to guide the response of the virtual agent and generate locomotion and gaze-based behaviors. Our overall approach allows the user to actively interact with tens of virtual agents from a first-person perspective in an immersive setting. We systematically evaluate our inference algorithm in controlled multi-agent simulation environments and highlight its ability to reliably and efficiently infer the hidden intent of a user's avatar even under noisy conditions. We quantitatively demonstrate the performance benefits of our approach in terms of reducing false inferences, as compared to a prior method. The results of our user evaluation show that 68.18% of participants reported feeling more comfortable in sharing the virtual environment with agents simulated with our algorithm as compared to a prior inference method, likely as a direct result of significantly fewer false inferences and more plausible responses from the virtual agents.", "cite": 1, "year": "2019"}], "name": "Manocha,Dinesh", "nodes": [{"id": "0"}, {"id": "1"}, {"id": "2"}, {"id": "3"}, {"id": "4"}, {"id": "5"}, {"id": "6"}, {"id": "7"}, {"id": "8"}, {"id": "9"}, {"id": "10"}, {"id": "11"}, {"id": "12"}, {"id": "13"}, {"id": "14"}, {"id": "15"}, {"id": "16"}, {"id": "17"}, {"id": "18"}, {"id": "19"}, {"id": "20"}, {"id": "21"}, {"id": "22"}, {"id": "23"}, {"id": "24"}, {"id": "25"}], "edgess": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "0", "target": "5"}, {"source": "0", "target": "6"}, {"source": "0", "target": "7"}, {"source": "0", "target": "8"}, {"source": "0", "target": "9"}, {"source": "0", "target": "10"}, {"source": "0", "target": "11"}, {"source": "0", "target": "12"}, {"source": "0", "target": "13"}, {"source": "0", "target": "14"}, {"source": "0", "target": "15"}, {"source": "0", "target": "16"}, {"source": "0", "target": "17"}, {"source": "0", "target": "18"}, {"source": "0", "target": "19"}, {"source": "0", "target": "20"}, {"source": "1", "target": "2"}, {"source": "1", "target": "3"}, {"source": "1", "target": "4"}, {"source": "2", "target": "3"}, {"source": "2", "target": "4"}, {"source": "2", "target": "5"}, {"source": "2", "target": "6"}, {"source": "3", "target": "4"}, {"source": "4", "target": "5"}, {"source": "4", "target": "6"}, {"source": "5", "target": "6"}, {"source": "7", "target": "8"}, {"source": "7", "target": "9"}, {"source": "7", "target": "10"}, {"source": "7", "target": "11"}, {"source": "8", "target": "9"}, {"source": "8", "target": "10"}, {"source": "8", "target": "11"}, {"source": "9", "target": "10"}, {"source": "9", "target": "11"}, {"source": "10", "target": "11"}, {"source": "10", "target": "21"}, {"source": "10", "target": "22"}, {"source": "12", "target": "13"}, {"source": "12", "target": "14"}, {"source": "13", "target": "14"}, {"source": "15", "target": "16"}, {"source": "15", "target": "17"}, {"source": "15", "target": "18"}, {"source": "16", "target": "17"}, {"source": "16", "target": "18"}, {"source": "17", "target": "23"}, {"source": "17", "target": "24"}, {"source": "17", "target": "25"}, {"source": "17", "target": "18"}, {"source": "18", "target": "23"}, {"source": "18", "target": "24"}, {"source": "18", "target": "25"}, {"source": "19", "target": "20"}], "year": 2019, "authorId": {"0": "Manocha,Dinesh", "1": "Randhavane,TanmayV", "2": "Bera,Aniket", "3": "Kubin,Emily", "4": "Gray,Kurt", "5": "Randhavane,Tanmay", "6": "Kapsaskis,Kyra", "7": "Ren,Jiaping", "8": "Xiang,Wei", "9": "Xiao,Yangxi", "10": "Yang,Ruigang", "11": "Jin,Xiaogang", "12": "Tian,Hao", "13": "Wang,Changbo", "14": "Zhang,Xinyu", "15": "Hu,Zhiming", "16": "Zhang,Congyi", "17": "Li,Sheng", "18": "Wang,Guoping", "19": "Narang,Sahil", "20": "Best,Andrew", "21": "Li,Wei", "22": "Gong,Huajun", "23": "Zhu,Kuixin", "24": "He,Xiaowei", "25": "Wang,Hongan"}}