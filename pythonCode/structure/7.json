{"edges": [[0, 1], [0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [1, 0], [2, 1], [2, 3], [2, 4], [2, 0], [3, 1], [3, 2], [3, 4], [3, 0], [4, 1], [4, 2], [4, 3], [4, 0]], "features": {"0": "0", "1": "1", "2": "2", "3": "3", "4": "4"}, "count": 1, "cite": 0, "position": 5.0, "connect": 1.0, "totalConnect": 2.0, "totalCount": 0.2, "totalCite": 0.0, "totalPosition": 5.0, "paper": [{"title": "Interactive Simulation of Scattering Effects in Participating Media Using a Neural Network Model.", "author": ["Ge,Liangsheng", "Wang,Beibei", "Wang,Lu", "Meng,Xiangxu", "Holzschuch,Nicolas"], "soname": "IEEE transactions on visualization and computer graphics", "DOI": "10.1109/TVCG.2019.2963015", "keywords": "", "abstract": "Rendering participating media is important to the creation of photorealistic images. Participating media has a translucent aspect that comes from light being scattered inside the material. For materials with a small mean-free-path (mfp), multiple scattering effects dominate. Simulating these effects is computationally intensive, as it requires tracking a large number of scattering events inside the material. Existing approaches precompute multiple scattering events inside the material and store the results in a table. During rendering time, this table is used to compute the scattering effects. While these methods are faster than explicit scattering computation, they incur higher storage costs. In this paper, we present a new representation for double and multiple scattering effects that uses a neural network model. The scattering response from all homogeneous participating media is encoded into a neural network in a preprocessing step. At run time, the neural network is then used to predict the double and multiple scattering effects. We demonstrate the effects combined with Virtual Ray Lights (VRL), although our approach can be integrated with other rendering algorithms. Our algorithm is implemented on GPU. Double and multiple scattering effects for the entire participating media space are encoded using only 23.6 KB of memory. Our method achieves 50 ms per frame in typical scenes and provides results almost identical to the reference.", "cite": 0, "year": "2019"}], "name": "Holzschuch,Nicolas", "nodes": [{"id": "0"}, {"id": "1"}, {"id": "2"}, {"id": "3"}, {"id": "4"}], "edgess": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "1", "target": "2"}, {"source": "1", "target": "3"}, {"source": "1", "target": "4"}, {"source": "2", "target": "3"}, {"source": "2", "target": "4"}, {"source": "3", "target": "4"}], "year": 2019, "authorId": {"0": "Holzschuch,Nicolas", "1": "Ge,Liangsheng", "2": "Wang,Beibei", "3": "Wang,Lu", "4": "Meng,Xiangxu"}}