{"edges": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [0, 1], [0, 10], [0, 11], [0, 12], [0, 13], [0, 14], [0, 15], [0, 16], [0, 1], [0, 10], [0, 12], [0, 2], [0, 17], [0, 18], [1, 2], [1, 3], [1, 0], [1, 10], [1, 11], [1, 12], [1, 0], [1, 10], [1, 0], [1, 12], [2, 1], [2, 3], [2, 0], [2, 17], [2, 0], [2, 18], [3, 1], [3, 2], [3, 0], [4, 5], [4, 6], [4, 0], [4, 7], [4, 8], [4, 9], [4, 19], [4, 20], [4, 21], [4, 22], [4, 23], [4, 24], [5, 4], [5, 6], [5, 0], [5, 7], [5, 8], [5, 9], [6, 4], [6, 5], [6, 0], [6, 7], [6, 8], [6, 9], [7, 4], [7, 5], [7, 6], [7, 0], [7, 8], [7, 9], [8, 4], [8, 5], [8, 6], [8, 0], [8, 7], [8, 9], [9, 4], [9, 5], [9, 6], [9, 0], [9, 7], [9, 8], [9, 25], [9, 26], [9, 27], [9, 28], [1, 2], [1, 3], [1, 0], [1, 10], [1, 11], [1, 12], [1, 0], [1, 10], [1, 0], [1, 12], [10, 1], [10, 11], [10, 12], [10, 0], [10, 1], [10, 0], [10, 12], [11, 1], [11, 10], [11, 12], [11, 0], [12, 1], [12, 10], [12, 11], [12, 0], [12, 1], [12, 10], [12, 0], [13, 0], [13, 14], [13, 15], [13, 16], [14, 13], [14, 0], [14, 15], [14, 16], [15, 13], [15, 0], [15, 14], [15, 16], [16, 13], [16, 0], [16, 14], [16, 15], [1, 2], [1, 3], [1, 0], [1, 10], [1, 11], [1, 12], [1, 0], [1, 10], [1, 0], [1, 12], [10, 1], [10, 11], [10, 12], [10, 0], [10, 1], [10, 0], [10, 12], [12, 1], [12, 10], [12, 11], [12, 0], [12, 1], [12, 10], [12, 0], [2, 1], [2, 3], [2, 0], [2, 17], [2, 0], [2, 18], [17, 2], [17, 0], [17, 18], [18, 2], [18, 17], [18, 0]], "features": {"0": "0", "1": "1", "2": "2", "3": "3", "4": "4", "5": "5", "6": "6", "7": "7", "8": "8", "9": "9", "10": "10", "11": "11", "12": "12", "13": "13", "14": "14", "15": "15", "16": "16", "17": "17", "18": "18", "19": "19", "20": "20", "21": "21", "22": "22", "23": "23", "24": "24", "25": "25", "26": "26", "27": "27", "28": "28"}, "count": 6, "cite": 37, "position": 3.5, "connect": 1.2777777777777777, "totalConnect": 2.7540983606557377, "totalCount": 0.41379310344827586, "totalCite": 4.083333333333333, "totalPosition": 5.333333333333333, "paper": [{"title": "Visualization and Visual Analysis of Ensemble Data: A Survey", "author": ["Wang,Junpeng", "Hazarika,Subhashis", "Li,Cheng", "Shen,Han-Wei"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2853721", "keywords": "", "abstract": "Over the last decade, ensemble visualization has witnessed a significant development due to the wide availability of ensemble data, and the increasing visualization needs from a variety of disciplines. From the data analysis point of view, it can be observed that many ensemble visualization works focus on the same facet of ensemble data, use similar data aggregation or uncertainty modeling methods. However, the lack of reflections on those essential commonalities and a systematic overview of those works prevents visualization researchers from effectively identifying new or unsolved problems and planning for further developments. In this paper, we take a holistic perspective and provide a survey of ensemble visualization. Specifically, we study ensemble visualization works in the recent decade, and categorize them from two perspectives: (1) their proposed visualization techniques; and (2) their involved analytic tasks. For the first perspective, we focus on elaborating how conventional visualization techniques (e.g., surface, volume visualization techniques) have been adapted to ensemble data; for the second perspective, we emphasize how analytic tasks (e.g., comparison, clustering) have been performed differently for ensemble data. From the study of ensemble visualization literature, we have also identified several research trends, as well as some future research opportunities.", "cite": 8, "year": "2019"}, {"title": "Extreme-Scale Stochastic Particle Tracing for Uncertain Unsteady Flow Visualization and Analysis", "author": ["Guo,Hanqi", "He,Wenbin", "Seo,Sangmin", "Shen,Han-Wei", "Constantinescu,EmilMihai", "Liu,Chunhui", "Peterka,Tom"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2856772", "keywords": "", "abstract": "We present an efficient and scalable solution to estimate uncertain transport behaviors-stochastic flow maps (SFMs)-for visualizing and analyzing uncertain unsteady flows. Computing flow maps from uncertain flow fields is extremely expensive because it requires many Monte Carlo runs to trace densely seeded particles in the flow. We reduce the computational cost by decoupling the time dependencies in SFMs so that we can process shorter sub time intervals independently and then compose them together for longer time periods. Adaptive refinement is also used to reduce the number of runs for each location. We parallelize over tasks-packets of particles in our design-to achieve high efficiency in MPI/thread hybrid programming. Such a task model also enables CPU/GPU coprocessing. We show the scalability on two supercomputers, Mira (up to 256K Blue Gene/Q cores) and Titan (up to 128K Opteron cores and 8K GPUs), that can trace billions of particles in seconds.", "cite": 0, "year": "2019"}, {"title": "DeepVID: Deep Visual Interpretation and Diagnosis for Image Classifiers via Knowledge Distillation", "author": ["Wang,Junpeng", "Gou,Liang", "Zhang,Wei", "Yang,Hao", "Shen,Han-Wei"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2019.2903943", "keywords": "", "abstract": "Deep Neural Networks (DNNs) have been extensively used in multiple disciplines due to their superior performance. However, in most cases, DNNs are considered as black-boxes and the interpretation of their internal working mechanism is usually challenging. Given that model trust is often built on the understanding of how a model works, the interpretation of DNNs becomes more important, especially in safety-critical applications (e.g., medical diagnosis, autonomous driving). In this paper, we propose DeepVID, a Deep learning approach to Visually Interpret and Diagnose DNN models, especially image classifiers. In detail, we train a small locally-faithful model to mimic the behavior of an original cumbersome DNN around a particular data instance of interest, and the local model is sufficiently simple such that it can be visually interpreted (e.g., a linear model). Knowledge distillation is used to transfer the knowledge from the cumbersome DNN to the small model, and a deep generative model (i.e., variational auto-encoder) is used to generate neighbors around the instance of interest. Those neighbors, which come with small feature variances and semantic meanings, can effectively probe the DNN's behaviors around the interested instance and help the small model to learn those behaviors. Through comprehensive evaluations, as well as case studies conducted together with deep learning experts, we validate the effectiveness of DeepVID.", "cite": 10, "year": "2019"}, {"title": "Visual Exploration of Neural Document Embedding in Information Retrieval: Semantics and Feature Selection", "author": ["Ji,Xiaonan", "Shen,Han-Wei", "Ritter,Alan", "Machiraju,Raghu", "Yen,Po-Yin"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2019.2903946", "keywords": "", "abstract": "Neural embeddings are widely used in language modeling and feature generation with superior computational power. Particularly, neural document embedding - converting texts of variable-length to semantic vector representations - has shown to benefit widespread downstream applications, e.g., information retrieval (IR). However, the black-box nature makes it difficult to understand how the semantics are encoded and employed. We propose visual exploration of neural document embedding to gain insights into the underlying embedding space, and promote the utilization in prevalent IR applications. In this study, we take an IR application-driven view, which is further motivated by biomedical IR in healthcare decision-making, and collaborate with domain experts to design and develop a visual analytics system. This system visualizes neural document embeddings as a configurable document map and enables guidance and reasoning; facilitates to explore the neural embedding space and identify salient neural dimensions (semantic features) per task and domain interest; and supports advisable feature selection (semantic analysis) along with instant visual feedback to promote IR performance. We demonstrate the usefulness and effectiveness of this system and present inspiring findings in use cases. This work will help designers/developers of downstream applications gain insights and confidence in neural document embedding, and exploit that to achieve more favorable performance in application domains.", "cite": 4, "year": "2019"}, {"title": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks", "author": ["Wang,Junpeng", "Gou,Liang", "Shen,Han-Wei", "Yang,Hao"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2864504", "keywords": "", "abstract": "Deep Q-Network (DON). as one type of deep reinforcement learning model. targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand. diagnose, and potentially improve DQN models.", "cite": 12, "year": "2019"}, {"title": "CoDDA: A Flexible Copula-based Distribution Driven Analysis Framework for Large-Scale Multivariate Data", "author": ["Hazarika,Subhashis", "Dutta,Soumya", "Shen,Han-Wei", "Chen,Jen-Ping"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2864801", "keywords": "", "abstract": "CoDDA (Copula-based Distribution Driven Analysis) is a flexible framework for large-scale multivariate datasets. A common strategy to deal with large-scale scientific simulation data is to partition the simulation domain and create statistical data summaries. Instead of storing the high-resolution raw data from the simulation, storing the compact statistical data summaries results in reduced storage overhead and alleviated I/O bottleneck. Such summaries, often represented in the form of statistical probability distributions, can serve various post-hoc analysis and visualization tasks. However, for multivariate simulation data using standard multivariate distributions for creating data summaries is not feasible. They are either storage inefficient or are computationally expensive to be estimated in simulation time (in situ) for large number of variables. In this work, using copula functions, we propose a flexible multivariate distribution-based data modeling and analysis framework that offers significant data reduction and can be used in an in situ environment. The framework also facilitates in storing the associated spatial information along with the multivariate distributions in an efficient representation. Using the proposed multivariate data summaries, we perform various multivariate post-hoc analyses like query-driven visualization and sampling-based visualization. We evaluate our proposed method on multiple real-world multivariate scientific datasets. To demonstrate the efficacy of our framework in an in situ environment, we apply it on a large-scale flow simulation.", "cite": 3, "year": "2019"}], "name": "Shen,Han-Wei", "nodes": [{"id": "0"}, {"id": "1"}, {"id": "2"}, {"id": "3"}, {"id": "4"}, {"id": "5"}, {"id": "6"}, {"id": "7"}, {"id": "8"}, {"id": "9"}, {"id": "10"}, {"id": "11"}, {"id": "12"}, {"id": "13"}, {"id": "14"}, {"id": "15"}, {"id": "16"}, {"id": "17"}, {"id": "18"}, {"id": "19"}, {"id": "20"}, {"id": "21"}, {"id": "22"}, {"id": "23"}, {"id": "24"}, {"id": "25"}, {"id": "26"}, {"id": "27"}, {"id": "28"}], "edgess": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "0", "target": "5"}, {"source": "0", "target": "6"}, {"source": "0", "target": "7"}, {"source": "0", "target": "8"}, {"source": "0", "target": "9"}, {"source": "0", "target": "10"}, {"source": "0", "target": "11"}, {"source": "0", "target": "12"}, {"source": "0", "target": "13"}, {"source": "0", "target": "14"}, {"source": "0", "target": "15"}, {"source": "0", "target": "16"}, {"source": "0", "target": "17"}, {"source": "0", "target": "18"}, {"source": "1", "target": "2"}, {"source": "1", "target": "3"}, {"source": "1", "target": "10"}, {"source": "1", "target": "11"}, {"source": "1", "target": "12"}, {"source": "2", "target": "3"}, {"source": "2", "target": "17"}, {"source": "2", "target": "18"}, {"source": "4", "target": "5"}, {"source": "4", "target": "6"}, {"source": "4", "target": "7"}, {"source": "4", "target": "8"}, {"source": "4", "target": "9"}, {"source": "4", "target": "19"}, {"source": "4", "target": "20"}, {"source": "4", "target": "21"}, {"source": "4", "target": "22"}, {"source": "4", "target": "23"}, {"source": "4", "target": "24"}, {"source": "5", "target": "6"}, {"source": "5", "target": "7"}, {"source": "5", "target": "8"}, {"source": "5", "target": "9"}, {"source": "6", "target": "7"}, {"source": "6", "target": "8"}, {"source": "6", "target": "9"}, {"source": "7", "target": "8"}, {"source": "7", "target": "9"}, {"source": "8", "target": "9"}, {"source": "9", "target": "25"}, {"source": "9", "target": "26"}, {"source": "9", "target": "27"}, {"source": "9", "target": "28"}, {"source": "10", "target": "11"}, {"source": "10", "target": "12"}, {"source": "11", "target": "12"}, {"source": "13", "target": "14"}, {"source": "13", "target": "15"}, {"source": "13", "target": "16"}, {"source": "14", "target": "15"}, {"source": "14", "target": "16"}, {"source": "15", "target": "16"}, {"source": "17", "target": "18"}], "year": 2019, "authorId": {"0": "Shen,Han-Wei", "1": "Wang,Junpeng", "2": "Hazarika,Subhashis", "3": "Li,Cheng", "4": "Guo,Hanqi", "5": "He,Wenbin", "6": "Seo,Sangmin", "7": "Constantinescu,EmilMihai", "8": "Liu,Chunhui", "9": "Peterka,Tom", "10": "Gou,Liang", "11": "Zhang,Wei", "12": "Yang,Hao", "13": "Ji,Xiaonan", "14": "Ritter,Alan", "15": "Machiraju,Raghu", "16": "Yen,Po-Yin", "17": "Dutta,Soumya", "18": "Chen,Jen-Ping", "19": "Tao,Jun", "20": "Imre,Martin", "21": "Wang,Chaoli", "22": "Chawla,NiteshV.", "23": "Sever,Gokhan", "24": "Kim,SeungHyun", "25": "Binyahib,Roba", "26": "Larsen,Matthew", "27": "Ma,Kwan-Liu", "28": "Childs,Hank"}}