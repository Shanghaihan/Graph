{"edges": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 3], [1, 0], [1, 2], [1, 3], [2, 0], [2, 1], [2, 3], [3, 6], [3, 7], [3, 0], [3, 1], [3, 2], [3, 0], [3, 4], [3, 5], [4, 0], [4, 5], [4, 3], [5, 0], [5, 4], [5, 3], [3, 6], [3, 7], [3, 0], [3, 1], [3, 2], [3, 0], [3, 4], [3, 5]], "features": {"0": "0", "1": "1", "2": "2", "3": "3", "4": "4", "5": "5", "6": "6", "7": "7"}, "count": 2, "cite": 3, "position": 1.0, "connect": 1.2, "totalConnect": 2.6153846153846154, "totalCount": 0.375, "totalCite": 1.0, "totalPosition": 3.6666666666666665, "paper": [{"title": "Fast Ray-Scene Intersection for Interactive Shadow Rendering with Thousands of Dynamic Lights", "author": ["Wang,Lili", "Liang,Xinglun", "Meng,Chunlei", "Popescu,Voicu"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2828422", "keywords": "", "abstract": "We present a method for the fast computation of the intersection between a ray and the geometry of a scene. The scene geometry is simplified with a 2D array of voxelizations computed from different directions, sampling the space of all possible directions. The 2D array of voxelizations is compressed using a vector quantization approach. The ray-scene intersection is approximated using the voxelization whose rows are most closely aligned with the ray. The voxelization row that contains the ray is looked up, the row is truncated to the extent of the ray using bit operations, and a truncated row with non-zero bits indicates that the ray intersects the scene. We support dynamic scenes with rigidly moving objects by building a separate 2D array of voxelizations for each type of object, and by using the same 2D array of voxelizations for all instances of an object type. We support complex dynamic scenes and scenes with deforming geometry by computing and rotating a single voxelization on the fly. We demonstrate the benefits of our method in the context of interactive rendering of scenes with thousands of moving lights, where we compare our method to ray tracing, to conventional shadow mapping, and to imperfect shadow maps.", "cite": 3, "year": "2019"}, {"title": "VR Exploration Assistance through Automatic Occlusion Removal", "author": ["Wang,Lili", "Wu,Jian", "Yang,Xuefeng", "Popescu,Voicu"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2019.2898782", "keywords": "", "abstract": "Virtual Reality (VR) applications allow a user to explore a scene intuitively through a tracked head-mounted display (HMD). However, in complex scenes, occlusions make scene exploration inefficient, as the user has to navigate around occluders to gain line of sight to potential regions of interest. When a scene region proves to be of no interest, the user has to retrace their path, and such a sequential scene exploration implies significant amounts of wasted navigation. Furthermore, as the virtual world is typically much larger than the tracked physical space hosting the VR application, the intuitive one-to-one mapping between the virtual and real space has to be temporarily suspended for the user to teleport or redirect in order to conform to the physical space constraints. In this paper we introduce a method for improving VR exploration efficiency by automatically constructing a multiperspective visualization that removes occlusions. For each frame, the scene is first rendered conventionally, the z-buffer is analyzed to detect horizontal and vertical depth discontinuities, the discontinuities are used to define disocclusion portals which are 3D scene rectangles for routing rays around occluders, and the disocclusion portals are used to render a multiperpsective image that alleviates occlusions. The user controls the multiperspective disocclusion effect, deploying and retracting it with small head translations. We have quantified the VR exploration efficiency brought by our occlusion removal method in a study where participants searched for a stationary target, and chased a dynamic target. Our method showed an advantage over conventional VR exploration in terms of reducing the navigation distance, the view direction rotation, the number of redirections, and the task completion time. These advantages did not come at the cost of a reduction in depth perception or situational awareness, or of an increase in simulator sickness.", "cite": 0, "year": "2019"}], "name": "Wang,Lili", "nodes": [{"id": "0"}, {"id": "1"}, {"id": "2"}, {"id": "3"}, {"id": "4"}, {"id": "5"}, {"id": "6"}, {"id": "7"}], "edgess": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "0", "target": "5"}, {"source": "1", "target": "2"}, {"source": "1", "target": "3"}, {"source": "2", "target": "3"}, {"source": "3", "target": "6"}, {"source": "3", "target": "7"}, {"source": "3", "target": "4"}, {"source": "3", "target": "5"}, {"source": "4", "target": "5"}], "year": 2019, "authorId": {"0": "Wang,Lili", "1": "Liang,Xinglun", "2": "Meng,Chunlei", "3": "Popescu,Voicu", "4": "Wu,Jian", "5": "Yang,Xuefeng", "6": "Andersen,Daniel", "7": "Villano,Peter"}}