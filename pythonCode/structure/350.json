{"edges": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 1], [0, 9], [1, 2], [1, 3], [1, 4], [1, 0], [1, 5], [1, 8], [1, 9], [1, 0], [2, 10], [2, 11], [2, 12], [2, 13], [2, 1], [2, 3], [2, 4], [2, 0], [2, 5], [2, 14], [2, 15], [2, 16], [3, 1], [3, 2], [3, 4], [3, 0], [3, 5], [4, 1], [4, 2], [4, 3], [4, 0], [4, 5], [5, 1], [5, 2], [5, 3], [5, 4], [5, 0], [6, 7], [6, 0], [7, 6], [7, 0], [8, 1], [8, 9], [8, 0], [1, 2], [1, 3], [1, 4], [1, 0], [1, 5], [1, 8], [1, 9], [1, 0], [9, 17], [9, 18], [9, 19], [9, 20], [9, 21], [9, 17], [9, 22], [9, 23], [9, 21], [9, 20], [9, 8], [9, 1], [9, 0]], "features": {"0": "0", "1": "1", "2": "2", "3": "3", "4": "4", "5": "5", "6": "6", "7": "7", "8": "8", "9": "9", "10": "10", "11": "11", "12": "12", "13": "13", "14": "14", "15": "15", "16": "16", "17": "17", "18": "18", "19": "19", "20": "20", "21": "21", "22": "22", "23": "23"}, "count": 3, "cite": 4, "position": 4.0, "connect": 1.1111111111111112, "totalConnect": 1.972972972972973, "totalCount": 0.4583333333333333, "totalCite": 3.8181818181818183, "totalPosition": 5.2727272727272725, "paper": [{"title": "Sparse Data Driven Mesh Deformation.", "author": ["Gao,Lin", "Lai,Yu-Kun", "Yang,Jie", "Ling-Xiao,Zhang", "Xia,Shihong", "Kobbelt,Leif"], "soname": "IEEE transactions on visualization and computer graphics", "DOI": "10.1109/TVCG.2019.2941200", "keywords": "", "abstract": "Example-based mesh deformation methods are powerful tools for realistic shape editing. However, existing techniques typically combine all the example deformation modes, which can lead to overfitting, i.e. using an overly complicated model to explain the user-specified deformation. This leads to implausible or unstable deformation results, including unexpected global changes outside the region of interest. To address this fundamental limitation, we propose a sparse blending method that automatically selects a smaller number of deformation modes to compactly describe the desired deformation. This along with a suitably chosen deformation basis including spatially localized deformation modes leads to significant advantages, including more meaningful, reliable, and efficient deformations because fewer and localized deformation modes are applied. To cope with large rotations, we develop a simple but effective representation based on polar decomposition of deformation gradients, which resolves the ambiguity of large global rotations using an as-consistent-as-possible global optimization. This simple representation has a closed form solution for derivatives, making it efficient for our sparse localized representation and thus ensuring interactive performance. Experimental results show that our method outperforms state-of-the-art data-driven mesh deformation methods, for both quality of results and efficiency.", "cite": 3, "year": "2019"}, {"title": "Realtime and Accurate 3D Eye Gaze Capture with DCNN-based Iris and Pupil Segmentation.", "author": ["Wang,Zhiyong", "Chai,Jinxiang", "Xia,Shihong"], "soname": "IEEE transactions on visualization and computer graphics", "DOI": "10.1109/TVCG.2019.2938165", "keywords": "", "abstract": "This paper presents a realtime and accurate method for 3D eye gaze tracking with a monocular RGB camera. Our key idea is to train a deep convolutional neural network(DCNN) that automatically extracts the iris and pupil pixels of each eye from input images. To achieve this goal, we combine the power of Unet\\cite{ronneberger2015u-net:} and Squeezenet\\cite{iandola2017squeezenet:} to train an efficient convolutional neural network for pixel classification. In addition, we track the 3D eye gaze state in the Maximum A Posteriori (MAP) framework, which sequentially searches for the most likely state of the 3D eye gaze at each frame. When eye blinking occurs, the eye gaze tracker can obtain an inaccurate result. We further extend the convolutional neural network for eye close detection in order to improve the robustness and accuracy of the eye gaze tracker. Our system runs in realtime on desktop PCs and smart phones. We have evaluated our system on live videos and Internet videos, and our results demonstrate that the system is robust and accurate for various genders, races, lighting conditions, poses, shapes and facial expressions. A comparison against Wang et al.[3] shows that our method advances the state of the art in 3D eye tracking using a single RGB camera.", "cite": 1, "year": "2019"}, {"title": "Temporal Upsampling of Depth Maps Using a Hybrid Camera", "author": ["Yuan,Ming-Ze", "Gao,Lin", "Fu,Hongbo", "Xia,Shihong"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2812879", "keywords": "", "abstract": "In recent years, consumer-level depth cameras have been adopted for various applications. However, they often produce depth maps at only a moderately high frame rate (approximately 30 frames per second), preventing them from being used for applications such as digitizing human performance involving fast motion. On the other hand, low-cost, high-frame-rate video cameras are available. This motivates us to develop a hybrid camera that consists of a high-frame-rate video camera and a low-frame-rate depth camera and to allow temporal interpolation of depth maps with the help of auxiliary color images. To achieve this, we develop a novel algorithm that reconstructs intermediate depth maps and estimates scene flow simultaneously. We test our algorithm on various examples involving fast, non-rigid motions of single or multiple objects. Our experiments show that our scene flow estimation method is more precise than a tracking-based method and the state-of-the-art techniques.", "cite": 0, "year": "2019"}], "name": "Xia,Shihong", "nodes": [{"id": "0"}, {"id": "1"}, {"id": "2"}, {"id": "3"}, {"id": "4"}, {"id": "5"}, {"id": "6"}, {"id": "7"}, {"id": "8"}, {"id": "9"}, {"id": "10"}, {"id": "11"}, {"id": "12"}, {"id": "13"}, {"id": "14"}, {"id": "15"}, {"id": "16"}, {"id": "17"}, {"id": "18"}, {"id": "19"}, {"id": "20"}, {"id": "21"}, {"id": "22"}, {"id": "23"}], "edgess": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "0", "target": "5"}, {"source": "0", "target": "6"}, {"source": "0", "target": "7"}, {"source": "0", "target": "8"}, {"source": "0", "target": "9"}, {"source": "1", "target": "2"}, {"source": "1", "target": "3"}, {"source": "1", "target": "4"}, {"source": "1", "target": "5"}, {"source": "1", "target": "8"}, {"source": "1", "target": "9"}, {"source": "2", "target": "10"}, {"source": "2", "target": "11"}, {"source": "2", "target": "12"}, {"source": "2", "target": "13"}, {"source": "2", "target": "3"}, {"source": "2", "target": "4"}, {"source": "2", "target": "5"}, {"source": "2", "target": "14"}, {"source": "2", "target": "15"}, {"source": "2", "target": "16"}, {"source": "3", "target": "4"}, {"source": "3", "target": "5"}, {"source": "4", "target": "5"}, {"source": "6", "target": "7"}, {"source": "8", "target": "9"}, {"source": "9", "target": "17"}, {"source": "9", "target": "18"}, {"source": "9", "target": "19"}, {"source": "9", "target": "20"}, {"source": "9", "target": "21"}, {"source": "9", "target": "22"}, {"source": "9", "target": "23"}], "year": 2019, "authorId": {"0": "Xia,Shihong", "1": "Gao,Lin", "2": "Lai,Yu-Kun", "3": "Yang,Jie", "4": "Ling-Xiao,Zhang", "5": "Kobbelt,Leif", "6": "Wang,Zhiyong", "7": "Chai,Jinxiang", "8": "Yuan,Ming-Ze", "9": "Fu,Hongbo", "10": "Zhang,Suiyun", "11": "Han,Zhizhong", "12": "Zwicker,Matthias", "13": "Zhang,Hui", "14": "Li,Kun", "15": "Yang,Jingyu", "16": "Guo,Daoliang", "17": "Xu,Pengfei", "18": "Yan,Guohang", "19": "Igarashi,Takeo", "20": "Tai,Chiew-Lan", "21": "Huang,Hui", "22": "Zheng,Youyi", "23": "Singh,Karan"}}