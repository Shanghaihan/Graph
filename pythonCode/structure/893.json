{"edges": [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 1], [0, 3], [1, 2], [1, 3], [1, 0], [1, 0], [1, 4], [1, 5], [1, 3], [2, 1], [2, 3], [2, 0], [3, 1], [3, 2], [3, 0], [3, 0], [3, 4], [3, 5], [3, 1], [4, 0], [4, 5], [4, 1], [4, 3], [5, 6], [5, 7], [5, 8], [5, 9], [5, 10], [5, 11], [5, 12], [5, 0], [5, 4], [5, 1], [5, 3], [1, 2], [1, 3], [1, 0], [1, 0], [1, 4], [1, 5], [1, 3], [3, 1], [3, 2], [3, 0], [3, 0], [3, 4], [3, 5], [3, 1]], "features": {"0": "0", "1": "1", "2": "2", "3": "3", "4": "4", "5": "5", "6": "6", "7": "7", "8": "8", "9": "9", "10": "10", "11": "11", "12": "12"}, "count": 2, "cite": 17, "position": 2.5, "connect": 1.4, "totalConnect": 2.65, "totalCount": 0.38461538461538464, "totalCite": 8.6, "totalPosition": 4.2, "paper": [{"title": "Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data", "author": ["Kale,Alex", "Nguyen,Francis", "Kay,Matthew", "Hullman,Jessica"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2864909", "keywords": "", "abstract": "Animated representations of outcomes drawn from distributions (hypothetical outcome plots, or HOPs) are used in the media and other public venues to communicate uncertainty. HOPs greatly improve multivariate probability estimation over conventional static uncertainty visualizations and leverage the ability of the visual system to quickly, accurately, and automatically process the summary statistical properties of ensembles. However, it is unclear how well HOPs support applied tasks resembling real world judgments posed in uncertainty communication. We identify and motivate an appropriate task to investigate realistic judgments of uncertainty in the public domain through a qualitative analysis of uncertainty visualizations in the news. We contribute two crowdsourced experiments comparing the effectiveness of HOPs, error bars, and line ensembles for supporting perceptual decision-making from visualized uncertainty. Participants infer which of two possible underlying trends is more likely to have produced a sample of time series data by referencing uncertainty visualizations which depict the two trends with variability due to sampling error. By modeling each participant's accuracy as a function of the level of evidence presented over many repeated judgments, we find that observers are able to correctly infer the underlying trend in samples conveying a lower level of evidence when using HOPs rather than static aggregate uncertainty visualizations as a decision aid. Modeling approaches like ours contribute theoretically grounded and richly descriptive accounts of user perceptions to visualization evaluation.", "cite": 5, "year": "2019"}, {"title": "In Pursuit of Error: A Survey of Uncertainty Visualization Evaluation", "author": ["Hullman,Jessica", "Qiao,Xiaoli", "Correll,Michael", "Kale,Alex", "Kay,Matthew"], "soname": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "DOI": "10.1109/TVCG.2018.2864889", "keywords": "", "abstract": "Understanding and accounting for uncertainty is critical to effectively reasoning about visualized data. However, evaluating the impact of an uncertainty visualization is complex due to the difficulties that people have interpreting uncertainty and the challenge of defining correct behavior with uncertainty information. Currently, evaluators of uncertainty visualization must rely on general purpose visualization evaluation frameworks which can be ill-equipped to provide guidance with the unique difficulties of assessing judgments under uncertainty. To help evaluators navigate these complexities, we present a taxonomy for characterizing decisions made in designing an evaluation of an uncertainty visualization. Our taxonomy differentiates six levels of decisions that comprise an uncertainty visualization evaluation: the behavioral targets of the study, expected effects from an uncertainty visualization, evaluation goals, measures, elicitation techniques, and analysis approaches. Applying our taxonomy to 86 user studies of uncertainty visualizations, we find that existing evaluation practice, particularly in visualization research, focuses on Performance and Satisfaction-based measures that assume more predictable and statistically-driven judgment behavior than is suggested by research on human judgment and decision making. We reflect on common themes in evaluation practice concerning the interpretation and semantics of uncertainty, the use of confidence reporting, and a bias toward evaluating performance as accuracy rather than decision quality. We conclude with a concrete set of recommendations for evaluators designed to reduce the mismatch between the conceptualization of uncertainty in visualization versus other fields.", "cite": 12, "year": "2019"}], "name": "Hullman,Jessica", "nodes": [{"id": "0"}, {"id": "1"}, {"id": "2"}, {"id": "3"}, {"id": "4"}, {"id": "5"}, {"id": "6"}, {"id": "7"}, {"id": "8"}, {"id": "9"}, {"id": "10"}, {"id": "11"}, {"id": "12"}], "edgess": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "0", "target": "5"}, {"source": "1", "target": "2"}, {"source": "1", "target": "3"}, {"source": "1", "target": "4"}, {"source": "1", "target": "5"}, {"source": "2", "target": "3"}, {"source": "3", "target": "4"}, {"source": "3", "target": "5"}, {"source": "4", "target": "5"}, {"source": "5", "target": "6"}, {"source": "5", "target": "7"}, {"source": "5", "target": "8"}, {"source": "5", "target": "9"}, {"source": "5", "target": "10"}, {"source": "5", "target": "11"}, {"source": "5", "target": "12"}], "year": 2019, "authorId": {"0": "Hullman,Jessica", "1": "Kale,Alex", "2": "Nguyen,Francis", "3": "Kay,Matthew", "4": "Qiao,Xiaoli", "5": "Correll,Michael", "6": "Sarikaya,Alper", "7": "Bartram,Lyn", "8": "Tory,Melanie", "9": "Fisher,Danyel", "10": "Li,Mingwei", "11": "Kindlmann,Gordon", "12": "Scheidegger,Carlos"}}